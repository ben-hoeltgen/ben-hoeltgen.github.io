---
title: "Which distribution were you sampled from? Towards a more tangible conception of data"
collection: publications
category: in-progress
permalink: /publication/2024-icml-causality
excerpt: 'We should, at least in social settings, model ML without true distributions.'
date: 2024-10-01
# venue: 'NeurIPS Worksho: Explainable AI Approaches for Debugging and Diagnosis'
# slidesurl: 'http://academicpages.github.io/files/slides1.pdf'
# paperurl: 'http://academicpages.github.io/files/paper1.pdf'
# bibtexurl: 'http://academicpages.github.io/files/bibtex1.bib'
citation: '<b>HÃ¶ltgen, B.</b>, Williamson, R.C.: &quot;Which distribution were you sampled from? Towards a more tangible conception of data.&quot; <i>ICML Workshop: Humans, Algorithmic Decision-Making and Society</i>. 2024.'
---
### Abstract:
Machine Learning research, as most of Statistics, heavily relies on the concept of a data-generating probability distribution. The standard presumption is that since data points are `sampled from' such a distribution, one can learn from observed data about this distribution and, thus, predict future data points which, it is presumed, are also drawn from it. Drawing on scholarship across disciplines, we here argue that this framework is not always a good model. Not only do such true probability distributions not exist; the framework can also be misleading and obscure both the choices made and the goals pursued in machine learning practice. We suggest an alternative framework that focuses on finite populations rather than abstract distributions; while classical learning theory can be left almost unchanged, it opens new opportunities, especially to model sampling. We compile these considerations into five reasons for modelling machine learning -- in some settings -- with finite populations rather than generative distributions, both to be more faithful to practice and to provide novel theoretical insights.

[**preprint**](https://arxiv.org/pdf/2407.17395?)
