---
title: "Reconsidering Fairness Through Unawareness From the Perspective of Model Multiplicity"
collection: publications
category: in-progress
permalink: /publication/2025-ftu-mm
excerpt: 'Excluding protected attributes can reduce disparate impact without reducing accuracy.'
date: 2024-10-01
# venue: 'NeurIPS Worksho: Explainable AI Approaches for Debugging and Diagnosis'
# slidesurl: 'http://academicpages.github.io/files/slides1.pdf'
# paperurl: 'http://academicpages.github.io/files/paper1.pdf'
# bibtexurl: 'http://academicpages.github.io/files/bibtex1.bib'
citation: '<b>HÃ¶ltgen, B.</b>, Oliver, N.: &quot;Reconsidering fairness through unawareness from the perspective of model multiplicity.&quot; <i>ArXiv preprint.</i> 2025.'
---
### Abstract:
Fairness through Unawareness (FtU) describes the idea that discrimination against demographic groups can be avoided by not considering group membership in the decisions or predictions. This idea has long been criticized in the machine learning literature as not being sufficient to ensure fairness. In addition, the use of additional features is typically thought to increase the accuracy of the predictions for all groups, so that FtU is sometimes thought to be detrimental to all groups. In this paper, we show both theoretically and empirically that FtU can reduce algorithmic discrimination without necessarily reducing accuracy. We connect this insight with the literature on Model Multiplicity, to which we contribute with novel theoretical and empirical results. Furthermore, we illustrate how, in a real-life application, FtU can contribute to the deployment of more equitable policies without losing efficacy. Our findings suggest that FtU is worth considering in practical applications, particularly in high-risk scenarios, and that the use of protected attributes such as gender in predictive models should be accompanied by a clear and well-founded justification.

[**preprint**](https://arxiv.org/pdf/2505.16638)
